# üöÄ Guide de D√©ploiement

## Architecture

- **PostgreSQL** : Installation native (performance maximale)
- **API** : Node.js natif (PM2 pour la gestion des processus)

## üì¶ D√©ploiement Production

### Pr√©requis

- Node.js 20+
- PostgreSQL 16+
- PM2 (optionnel mais recommand√©)

### √âtapes de d√©ploiement

```bash
# 1. Installer PostgreSQL
# Ubuntu/Debian
sudo apt update
sudo apt install postgresql postgresql-contrib

# macOS
brew install postgresql@16
brew services start postgresql@16

# 2. Cr√©er la base de donn√©es
sudo -u postgres psql -c "CREATE DATABASE scrappingscan;"
sudo -u postgres psql -c "CREATE USER scrappingscan WITH PASSWORD 'votre_password_securise';"
sudo -u postgres psql -c "GRANT ALL PRIVILEGES ON DATABASE scrappingscan TO scrappingscan;"

# 3. Cloner le projet
git clone https://github.com/votre-repo/ScrappingScan.git
cd ScrappingScan

# 4. Installer les d√©pendances
npm install

# 5. Configurer l'environnement
cp .env.local .env
nano .env  # Modifier DATABASE_URL avec votre config

# 6. Appliquer les migrations
npm run prisma:migrate

# 7. Build
npm run build

# 8. D√©marrer avec PM2
npm install -g pm2
pm2 start dist/server.js --name scrappingscan-api
pm2 save
pm2 startup
```

**Railway :**

```bash
# 1. Cr√©er un projet sur Railway
# 2. Ajouter un service PostgreSQL
# 3. Connecter votre repo GitHub
# 4. Variables d'environnement :
DATABASE_URL=<fourni par Railway>
PORT=3000
NODE_ENV=production

# Railway g√®re automatiquement :
# - npm install
# - npm run build
# - npm run start
```

**Vercel (avec Vercel Postgres) :**

```bash
# 1. Installer Vercel CLI
npm i -g vercel

# 2. D√©ployer
vercel

# 3. Ajouter Vercel Postgres dans le dashboard
# 4. Les variables DATABASE_URL sont ajout√©es automatiquement
```

## üîí S√©curit√© Production

### Variables d'environnement

```bash
# .env (production)
DATABASE_URL="postgresql://user:password@host:5432/db"
PORT=3000
NODE_ENV=production
CORS_ORIGIN=https://votre-frontend.com  # Restreindre CORS
```

### Nginx Reverse Proxy

```nginx
server {
    listen 80;
    server_name votre-domaine.com;

    location / {
        proxy_pass http://localhost:3000;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_cache_bypass $http_upgrade;
    }
}
```

### SSL avec Let's Encrypt

```bash
sudo apt install certbot python3-certbot-nginx
sudo certbot --nginx -d votre-domaine.com
```

## üìä Monitoring

### PM2 Monitoring

```bash
# Logs
pm2 logs scrappingscan-api

# Status
pm2 status

# Red√©marrer
pm2 restart scrappingscan-api

# Monitoring web
pm2 plus
```

### PostgreSQL Backup

```bash
# Backup automatique quotidien
crontab -e

# Ajouter :
0 2 * * * pg_dump -U scrappingscan scrappingscan > /backup/db_$(date +\%Y\%m\%d).sql
```

## üîÑ Mise √† jour

```bash
# 1. Pull les changements
git pull

# 2. Installer nouvelles d√©pendances
npm install

# 3. Migrations si n√©cessaire
npm run prisma:migrate

# 4. Rebuild
npm run build

# 5. Red√©marrer
pm2 restart scrappingscan-api
```

## ‚ö†Ô∏è Notes Importantes

### Scraping en Production

**Le scraping avec Cloudflare est probl√©matique en production.**

Solutions recommand√©es :

1. **Scraper en local, sync la DB**

   ```bash
   # Local
   npm run dev:local
   curl POST /api/scraping/add-manga

   # Puis sync la DB vers prod
   pg_dump local_db | psql production_db
   ```

2. **VPS d√©di√© au scraping**
   - Un serveur uniquement pour scraper
   - Sync p√©riodique vers la prod

3. **Service externe**
   - ScraperAPI, Bright Data, etc.
   - Meilleure fiabilit√© contre Cloudflare

### Limites de Ressources

**Puppeteer consomme beaucoup de RAM :**

- Minimum 2GB RAM recommand√©
- Limiter le nombre de scraping simultan√©s
- Utiliser une queue (Bull, BullMQ) si volume √©lev√©
